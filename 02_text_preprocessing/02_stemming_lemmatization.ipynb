{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark> Stemming using nltk\n",
    "    \n",
    "    Reducing words to their basic form or root\n",
    "    There are several different algorithms for stemming, including the Porter stemmer, Snowball stemmer, and the Lancaster stemmer. \n",
    "    To build a robust model, it is essential to normalize text by removing repetition and transforming words to their base form through stemming.\n",
    "    Every word can be represented as a sequence of consonants and vowels.\n",
    "        CVCV…C\n",
    "        CVCV…V\n",
    "        VCVC…C\n",
    "        VCVC…V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <mark> Types of stemmers\n",
    "\n",
    "**PORTER**\n",
    "    \n",
    "    A suffix stripping algorithm.\n",
    "    It uses predefined rules to strip words\n",
    "    applies more than 50 rules, grouped in 5 steps\n",
    "    most commonly used\n",
    "    limited to English words.\n",
    "    Need not give meaningful words\n",
    "    It is based on the idea that the suffixes in the English language are made up of a combination of smaller and simpler suffixes.\n",
    "    \n",
    "**LANCASTER**\n",
    "    \n",
    "    one of the most aggressive stemmers as it tends to over stem many words\n",
    "    more strict rules\n",
    "    more than 100 rules, around double that of Porter stemmer\n",
    "        consists of a set of rules where each rule specifies either deletion or replacement of an ending\n",
    "        some rules are restricted to intact words, and some rules are applied iteratively as the word goes through them\n",
    "    The stemmer is really faster, but the algorithm is really confusing when dealing with small words\n",
    "\n",
    "**SNOWBALL**\n",
    "    \n",
    "    multi-lingual stemmer\n",
    "    way more aggressive than Porter Stemmer and is also referred to as Porter2\n",
    "    having greater computational speed than porter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <mark> There are mainly two errors in stemming – \n",
    "\n",
    "**over-stemming**\n",
    "    \n",
    "    when the stemmer is too aggressive in removing suffixes or when it does not consider the context of the word\n",
    "    stemmer produces a root form that is not a valid word or is not the correct root form of a word\n",
    "\n",
    "**under-stemming**\n",
    "    \n",
    "    when the stemmer is not aggressive enough in removing suffixes\n",
    "    \n",
    "In some cases, using a lemmatizer instead of a stemmer may be a better solution as it takes into account the context of the word, making it less prone to errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(dir(nltk.stem))\n",
    "\n",
    "# ['arlstem', 'arlstem2', 'cistem', 'isri', 'lancaster', 'porter', 'regexp', 'rslp', 'snowball', 'util', 'wordnet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run --> run\n",
      "runner --> runner\n",
      "running --> run\n",
      "ran --> ran\n",
      "runs --> run\n",
      "easily --> easili\n",
      "fairly --> fairli\n"
     ]
    }
   ],
   "source": [
    "words = ['run', 'runner', 'running', 'ran', 'runs', 'easily', 'fairly']\n",
    "\n",
    "for word in words:\n",
    "    print(word+' --> '+ p_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run --> run\n",
      "runner --> runner\n",
      "running --> run\n",
      "ran --> ran\n",
      "runs --> run\n",
      "easily --> easili\n",
      "fairly --> fair\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "s_stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "words = ['run', 'runner', 'running', 'ran', 'runs', 'easily', 'fairly']\n",
    "\n",
    "for word in words:\n",
    "    print(word+' --> ' + s_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generous --> generous\n",
      "generous --> gener \n",
      "\n",
      "generation --> generat\n",
      "generation --> gener \n",
      "\n",
      "generously --> generous\n",
      "generously --> gener \n",
      "\n",
      "generate --> generat\n",
      "generate --> gener \n",
      "\n"
     ]
    }
   ],
   "source": [
    "words = ['generous', 'generation', 'generously', 'generate']\n",
    "\n",
    "for word in words:\n",
    "    print(word+' --> ' + s_stemmer.stem(word))\n",
    "    print(word+' --> ' + p_stemmer.stem(word), '\\n')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer , LancasterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "ls =  LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is ---> is ---> is\n",
      "was ---> wa ---> was\n",
      "be ---> be ---> be\n",
      "been ---> been ---> been\n",
      "are ---> are ---> ar\n",
      "were ---> were ---> wer\n"
     ]
    }
   ],
   "source": [
    "words = [\"is\", \"was\", \"be\", \"been\", \"are\", \"were\"]\n",
    "\n",
    "for w in words:\n",
    "    print(f'{w} ---> {ps.stem(w)} ---> {ls.stem(w)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book ---> book ---> book\n",
      "booking ---> book ---> book\n",
      "booked ---> book ---> book\n",
      "books ---> book ---> book\n",
      "booker ---> booker ---> book\n",
      "bookstore ---> bookstor ---> bookst\n"
     ]
    }
   ],
   "source": [
    "words = [\"book\",\"booking\",\"booked\",\"books\",\"booker\",\"bookstore\"]\n",
    "\n",
    "for w in words:\n",
    "    print(f'{w} ---> {ps.stem(w)} ---> {ls.stem(w)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Word       Porter Stemmer      lancaster Stemmer   \n",
      "Artificial          artifici            art                 \n",
      "intelligence        intellig            intellig            \n",
      "(                   (                   (                   \n",
      "AI                  ai                  ai                  \n",
      ")                   )                   )                   \n",
      "is                  is                  is                  \n",
      "the                 the                 the                 \n",
      "intelligence        intellig            intellig            \n",
      "of                  of                  of                  \n",
      "machines            machin              machin              \n",
      "or                  or                  or                  \n",
      "software            softwar             softw               \n",
      ",                   ,                   ,                   \n",
      "as                  as                  as                  \n",
      "opposed             oppos               oppos               \n",
      "to                  to                  to                  \n",
      "the                 the                 the                 \n",
      "intelligence        intellig            intellig            \n",
      "of                  of                  of                  \n",
      "human               human               hum                 \n",
      "beings              be                  being               \n",
      "or                  or                  or                  \n",
      "animals             anim                anim                \n",
      ".                   .                   .                   \n"
     ]
    }
   ],
   "source": [
    "sentence = 'Artificial intelligence (AI) is the intelligence of machines or software, as opposed to the intelligence of human beings or animals.'\n",
    "word_list = word_tokenize(sentence)\n",
    "\n",
    "print(\"{0:20}{1:20}{2:20}\".format(\"Original Word\", \"Porter Stemmer\", \"lancaster Stemmer\", '\\n'))\n",
    "\n",
    "for word in word_list:\n",
    "    print(\"{0:20}{1:20}{2:20}\".format(word, ps.stem(word), ls.stem(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Word       Porter Stemmer      lancaster Stemmer   \n",
      "friend              friend              friend              \n",
      "friendship          friendship          friend              \n",
      "friends             friend              friend              \n",
      "friendships         friendship          friend              \n",
      "stabil              stabil              stabl               \n",
      "destabilize         destabil            dest                \n",
      "misunderstanding    misunderstand       misunderstand       \n",
      "railroad            railroad            railroad            \n",
      "moonlight           moonlight           moonlight           \n",
      "football            footbal             footbal             \n"
     ]
    }
   ],
   "source": [
    "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\", \"stabil\", \"destabilize\", \"misunderstanding\",\n",
    "             \"railroad\", \"moonlight\", \"football\"]\n",
    "\n",
    "print(\"{0:20}{1:20}{2:20}\".format(\"Original Word\", \"Porter Stemmer\", \"lancaster Stemmer\"))\n",
    "\n",
    "for word in word_list:\n",
    "    print(\"{0:20}{1:20}{2:20}\".format(word, ps.stem(word), ls.stem(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark> stemming using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n",
      "Saturn \t NOUN \t saturn\n",
      "is \t AUX \t be\n",
      "the \t DET \t the\n",
      "sixth \t ADJ \t sixth\n",
      "planet \t NOUN \t planet\n",
      "from \t ADP \t from\n",
      "the \t DET \t the\n",
      "Sun \t PROPN \t Sun\n",
      "and \t CCONJ \t and\n",
      "the \t DET \t the\n",
      "second \t ADV \t second\n",
      "- \t PUNCT \t -\n",
      "largest \t ADJ \t large\n",
      "in \t ADP \t in\n",
      "the \t DET \t the\n",
      "Solar \t PROPN \t Solar\n",
      "System \t PROPN \t System\n",
      ", \t PUNCT \t ,\n",
      "after \t ADP \t after\n",
      "Jupiter \t PROPN \t Jupiter\n",
      ". \t PUNCT \t .\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(u\"Saturn is the sixth planet from the Sun and the second-largest in the Solar System, after Jupiter.\")\n",
    "print(type(doc1))\n",
    "\n",
    "for token in doc1:\n",
    "    print(token.text, '\\t', token.pos_, '\\t', token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_lemmas(text):\n",
    "    for token in text:\n",
    "        print(f'{token.text:{12}} {token.pos_:{6}} {token.lemma:<{22}} {token.lemma_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupiter      PROPN  2889603257431922515    Jupiter\n",
      "is           AUX    10382539506755952630   be\n",
      "the          DET    7425985699627899538    the\n",
      "fifth        ADJ    4490412142941298567    fifth\n",
      "planet       NOUN   2468667252130234137    planet\n",
      "from         ADP    7831658034963690409    from\n",
      "the          DET    7425985699627899538    the\n",
      "Sun          PROPN  2663045040185303238    Sun\n",
      ".            PUNCT  12646065887601541794   .\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(u\"Jupiter is the fifth planet from the Sun.\")\n",
    "\n",
    "show_lemmas(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <mark> Lemmatization using nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "cactus\n",
      "radius\n",
      "foot\n",
      "speechless\n",
      "runner\n"
     ]
    }
   ],
   "source": [
    "words = [\"cats\", \"cacti\", \"radii\", \"feet\", \"speechless\", 'runner']\n",
    "\n",
    "for word in words : \n",
    "    print(lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enjoying\n",
      "enjoy\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"enjoying\", \"n\"))\n",
    "print(lemmatizer.lemmatize(\"enjoying\", 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma(pos='noun')   Lemma(pos='verb')   \n",
      "We                  We                  We                  \n",
      "are                 are                 be                  \n",
      "a                   a                   a                   \n",
      "non-profit          non-profit          non-profit          \n",
      "organisation        organisation        organisation        \n",
      "focused             focused             focus               \n",
      "on                  on                  on                  \n",
      "dialogue            dialogue            dialogue            \n",
      "and                 and                 and                 \n",
      "advocacy            advocacy            advocacy            \n",
      "and                 and                 and                 \n",
      "memory              memory              memory              \n",
      "and                 and                 and                 \n",
      "legacy              legacy              legacy              \n",
      "work                work                work                \n",
      "founded             founded             found               \n",
      "by                  by                  by                  \n",
      "enjoying            enjoying            enjoy               \n",
      "people              people              people              \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"We are a non-profit organisation focused on dialogue and advocacy, and memory and legacy work, founded by enjoying people\"\n",
    "punctuations=\"?:!.,;\"\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "\n",
    "# sentence_words\n",
    "print(\"{0:20}{1:20}{2:20}\".format(\"Word\", \"Lemma(pos='noun')\", \"Lemma(pos='verb')\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}{2:20}\".format(word, wordnet_lemmatizer.lemmatize(word), wordnet_lemmatizer.lemmatize(word, pos='v')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is\n",
      "wa\n",
      "be\n",
      "been\n",
      "are\n",
      "were\n"
     ]
    }
   ],
   "source": [
    "words = [\"is\", \"was\", \"be\", \"been\", \"are\", \"were\"]\n",
    "\n",
    "for word in words : \n",
    "    print(lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foot\n",
      "radius\n",
      "men\n",
      "child\n",
      "carpenter\n",
      "fighter\n"
     ]
    }
   ],
   "source": [
    "words = [\"feet\", \"radii\", \"men\", \"children\", \"carpenter\", \"fighter\"]\n",
    "for word in words : \n",
    "    print(lemmatizer.lemmatize(word,'n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
